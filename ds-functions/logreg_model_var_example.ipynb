{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monthly Buyer Likelihood model\n",
    "___\n",
    "Goal: To see if we can understand the variable list better and reduce the error rate for this problem statement\n",
    "\n",
    "Methodology for First Pass:\n",
    "\n",
    "Create an elastic net linear regression to help parry down the variable list that we start with and better understand a good baseline error rate\n",
    "Run the field list through a logistic regression to set up baseline error rate to compare adding variables to.\n",
    "Add in the variables 1 by 1, order all additive variables by how much they reduce the RMSE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c7440a7a631f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcat_variable_level_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def cat_variable_level_check(trainx, testx):\n",
    "\n",
    "    train_level_cols = trainx.columns[(trainx.dtypes == 'object') | (trainx.dtypes == 'category')]\n",
    "\n",
    "    test_level_cols = testx.columns[(testx.dtypes == 'object') | (trainx.dtypes == 'category')]\n",
    "\n",
    "    if set(train_level_cols) == set(test_level_cols):\n",
    "        print(\"Categorical variable columns match, checking for missing levels within the train data set\")\n",
    "        \n",
    "        for idx, val in enumerate(test_level_cols):\n",
    "            replace_list = np.setdiff1d(testx[val].values.categories, trainx[val].values.categories)\n",
    "            if all(replace_list == 0) == False:\n",
    "                print(\"We are missing some levels either the test or train set for the column:\", val, \" Please fix before continuing.\")\n",
    "                \n",
    "                return False\n",
    "            else:\n",
    "                print(\"All values match for the train and test categorical columns. Move to all column matching\")\n",
    "                \n",
    "                return True\n",
    "    else:\n",
    "        print(\"The Categorical Variable set does not match, please check that the columns are the same\")\n",
    "        \n",
    "        return False\n",
    "    if set(trainx.columns) == set(testx.columns):\n",
    "        print(\"All columns and levels match in the two data sets.\")\n",
    "        \n",
    "        return True\n",
    "    else:\n",
    "        print(\"The columns in the train data set and test data set do not match, please go check.\")\n",
    "        \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset(trainx, testx):\n",
    "    \n",
    "    if set(trainx.columns) == set(testx.columns):\n",
    "        norm_trainx = pd.DataFrame().reindex_like(trainx)\n",
    "        norm_testx = pd.DataFrame().reindex_like(testx)\n",
    "\n",
    "        for idx, datatype in enumerate(trainx.dtypes):\n",
    "            \n",
    "            if datatype == 'int64':\n",
    "                col_name = trainx.columns[idx]\n",
    "                mean_train = trainx[col_name].mean()\n",
    "                std_train = trainx[col_name].std()\n",
    "                if std_train == 0:\n",
    "                    norm_trainx[col_name] = trainx[col_name]\n",
    "                    norm_testx[col_name] = testx[col_name]\n",
    "                else:    \n",
    "                    norm_trainx[col_name] = (trainx[col_name] - mean_train)/std_train\n",
    "                    norm_testx[col_name] = (testx[col_name] - mean_train)/std_train\n",
    "            \n",
    "            if datatype == 'float64':\n",
    "                col_name = trainx.columns[idx]\n",
    "                mean_train = trainx[col_name].mean()\n",
    "                std_train = trainx[col_name].std()\n",
    "                if std_train == 0:\n",
    "                    norm_trainx[col_name] = trainx[col_name]\n",
    "                    norm_testx[col_name] = testx[col_name]\n",
    "                else:    \n",
    "                    norm_trainx[col_name] = (trainx[col_name] - mean_train)/std_train\n",
    "                    norm_testx[col_name] = (testx[col_name] - mean_train)/std_train\n",
    "            \n",
    "            else:\n",
    "                col_name = trainx.columns[idx]\n",
    "                norm_trainx[col_name] = trainx[col_name]\n",
    "                norm_testx[col_name] = testx[col_name]\n",
    "                \n",
    "    elif set(trainx.columns) != set(testx.columns):\n",
    "        print(\"Columns are not the same between train and test dataset, please make sure the columns match\")\n",
    "                \n",
    "    return norm_trainx, norm_testx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def logreg_create_model_image(trainx, trainy, save, model_name='None'):\n",
    "    \n",
    "    logreg = LogisticRegression(\n",
    "        penalty = 'elasticnet',\n",
    "        max_iter = 1000\n",
    "    )\n",
    "    ohe_trainx =  pd.get_dummies(trainx)\n",
    "    logreg.fit(ohe_trainx, trainy)\n",
    "    \n",
    "    if save == True:\n",
    "        path = '/Users/agoyal/Documents/Projects/model_images/'\n",
    "        filename = path + model_name\n",
    "        file = open(filename, 'wb')\n",
    "        pickle.dump(logreg, file)\n",
    "        file.close()\n",
    "        \n",
    "    return print(\"file saved: model name is \" + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1910c9cf7f9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import math\n",
    "\n",
    "def logreg_coefs_and_test_rmse(trainx, trainy, testx, testy):\n",
    "    \n",
    "    norm_trainx, norm_testx = normalize_dataset(trainx, testx)\n",
    "    \n",
    "    logreg = LogisticRegression(\n",
    "        penalty = 'l2',\n",
    "        max_iter = 1000\n",
    "    )\n",
    "    \n",
    "    ohe_trainx =  pd.get_dummies(trainx)\n",
    "    ohe_testx =  pd.get_dummies(testx)\n",
    "    logreg.fit(ohe_trainx, trainy)\n",
    "    coeff_output = {\"Feature\": ohe_trainx.columns, \"estCoeff\": logreg.coef_.tolist()[0], \"Magnitude\": abs(logreg.coef_).tolist()[0]}\n",
    "\n",
    "    coeffs = pd.DataFrame(coeff_output).sort_values(\"Magnitude\", ascending = 0)\n",
    "    \n",
    "    pred_df = pd.DataFrame(testy).reset_index(drop=True)\n",
    "\n",
    "    pred_df.columns = ['act_y']\n",
    "    \n",
    "    pred_df['pred_y'] = pd.DataFrame(logreg.predict_proba(ohe_testx))[1]\n",
    "\n",
    "    pred_df['error_sq'] = (pred_df['pred_y'] - pred_df['act_y']) * (pred_df['pred_y'] - pred_df['act_y'])\n",
    "    \n",
    "    lm_rmse = math.sqrt((pred_df['error_sq'].sum()/pred_df['error_sq'].size))\n",
    "\n",
    "    return coeffs, lm_rmse, pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-b487f6a70e87>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mElasticNet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0melastic_net_coefs_and_test_rmse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml1_ratio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m.0001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "import math\n",
    "\n",
    "def elastic_net_coefs_and_test_rmse(trainx, trainy, testx, testy, alpha = .5, l1_ratio = .5, max_iter = 1000, tol = .0001):\n",
    "    \n",
    "    norm_trainx, norm_testx = normalize_dataset(trainx, testx)\n",
    "    \n",
    "    regr = ElasticNet(\n",
    "        alpha = alpha,\n",
    "        random_state = 0,\n",
    "        copy_X = True,\n",
    "        fit_intercept = True,\n",
    "        l1_ratio = l1_ratio,\n",
    "        max_iter = max_iter,\n",
    "        normalize = False,\n",
    "        positive = False,\n",
    "        precompute = False,\n",
    "        selection = 'cyclic',\n",
    "        tol = tol,\n",
    "        warm_start = False\n",
    "    )\n",
    "    \n",
    "    ohe_trainx =  pd.get_dummies(trainx)\n",
    "    ohe_testx =  pd.get_dummies(testx)\n",
    "    \n",
    "    \n",
    "    regr.fit(ohe_trainx, trainy)\n",
    "    coeff_output = {\"Feature\": ohe_trainx.columns, \"estCoeff\": regr.coef_.tolist(), \"Magnitude\": abs(regr.coef_).tolist()}\n",
    "\n",
    "    coeffs = pd.DataFrame(coeff_output).sort_values(\"Magnitude\", ascending = 0)\n",
    "    \n",
    "    pred_df = pd.DataFrame(testy).reset_index(drop=True)\n",
    "\n",
    "    pred_df.columns = ['act_y']\n",
    "    \n",
    "    pred_df['pred_y'] = pd.DataFrame(regr.predict_proba(ohe_testx))[1]\n",
    "\n",
    "    pred_df['error_sq'] = (pred_df['pred_y'] - pred_df['act_y']) * (pred_df['pred_y'] - pred_df['act_y'])\n",
    "    \n",
    "    en_rmse = math.sqrt((pred_df['error_sq'].sum()/pred_df['error_sq'].size))\n",
    "\n",
    "    return coeffs, en_rmse, pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to use these functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "pl_df_train = pl_df[pl_df['current_month'] < '2020-06-01']\n",
    "pl_df_test = pl_df[pl_df['current_month'] >= '2020-06-01']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Variable list for model\n",
    "\n",
    "Will add all fields within the elastic net model to widdle down the field list\n",
    "Once we have the fields that seem to make the most difference, we can start there and add fields in a more controlled manner to see what actually decreases the RMSE/improves the AUC and test set predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_list = [\n",
    "    'growth_segment',\n",
    "    'gender_class',\n",
    "    'mon_cur',\n",
    "    'months_since_join',\n",
    "    'months_since_last_purchase',\n",
    "    'sellers_bought_from',\n",
    "    'pm_sellers_bought_from',\n",
    "    'py_sellers_bought_from',\n",
    "    'offers',\n",
    "    'pm_offers',\n",
    "    'py_offers',\n",
    "    'comp_offers',\n",
    "    'pm_comp_offers',\n",
    "    'py_comp_offers',\n",
    "    'orders',\n",
    "    'pm_orders',\n",
    "    'py_orders',\n",
    "    'gmv',\n",
    "    'pm_gmv',\n",
    "    'py_gmv',\n",
    "    'listings_viewed',\n",
    "    'pm_listings_viewed',\n",
    "    'brand_browses',\n",
    "    'pm_brand_browses',\n",
    "    'blocked_by',\n",
    "    'pm_blocked_by',\n",
    "    'blocked',\n",
    "    'pm_blocked',\n",
    "    'users_followed',\n",
    "    'pm_users_followed',\n",
    "    'days_active',\n",
    "    'pm_days_active',\n",
    "    'likes',\n",
    "    'pm_likes',\n",
    "    'comments',\n",
    "    'pm_comments'\n",
    "]\n",
    "\n",
    "y_var = ['y_var']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train and test\n",
    "\n",
    "```\n",
    "pl_df_train_x = pl_df_train[field_list]\n",
    "pl_df_train_y = pl_df_train[y_var]\n",
    "\n",
    "pl_df_test_x = pl_df_test[field_list]\n",
    "pl_df_test_y = pl_df_test[y_var]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Run first Elastic Net to get base model\n",
    "\n",
    "```\n",
    "ohe_train_x = pd.get_dummies(pl_df_train_x)\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import math\n",
    "\n",
    "regr = ElasticNet(\n",
    "    alpha = .5,\n",
    "    random_state = 0,\n",
    "    copy_X = True,\n",
    "    fit_intercept = True,\n",
    "    l1_ratio = .5,\n",
    "    max_iter = 1000,\n",
    "    normalize = False,\n",
    "    positive = False,\n",
    "    precompute = False,\n",
    "    selection = 'cyclic',\n",
    "    tol = .0001,\n",
    "    warm_start = False\n",
    ")\n",
    "\n",
    "regr.fit(ohe_train_x, pl_df_train_y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort coefficients for the model\n",
    "```\n",
    "coeff_output = {\"Feature\": ohe_train_x.columns, \"estCoeff\": regr.coef_.tolist(), \"Magnitude\": abs(regr.coef_).tolist()}\n",
    "\n",
    "coeffs = pd.DataFrame(coeff_output).sort_values(\"Magnitude\", ascending = 0)\n",
    "coeffs = coeffs[coeffs['estCoeff'] > 0]\n",
    "\n",
    "\n",
    "coeffs['Feature'].tolist()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: First Iteration of Model\n",
    "___\n",
    "This should increase the efficacy of the base model and make it easier to understand what variables will actually reduce the error rate over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take the first logistic regression based on the base variables\n",
    "\n",
    "```\n",
    "logreg_field_list = coeffs['Feature'].tolist()\n",
    "\n",
    "pl_df_train_x = pl_df_train[logreg_field_list]\n",
    "pl_df_train_y = pl_df_train[y_var]\n",
    "\n",
    "pl_df_test_x = pl_df_test[logreg_field_list]\n",
    "pl_df_test_y = pl_df_test[y_var]\n",
    "\n",
    "coefs, rmse, pred_df = logreg_coefs_and_test_rmse(pl_df_train_x, pl_df_train_y, pl_df_test_x, pl_df_test_y)\n",
    "\n",
    "pred_df = pred_df.sort_values(\"pred_y\", ascending = 0).reset_index(drop = True)\n",
    "pred_df['index'] = pred_df.index\n",
    "pred_df['Quantile'] = pd.qcut(pred_df['index'], q = 10)\n",
    "\n",
    "pred_df.groupby('Quantile').agg(\n",
    "    {\n",
    "        'act_y':'sum',\n",
    "        'pred_y':'sum'\n",
    "    }\n",
    ").reset_index(drop = True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Test addition of other variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up for running the loop\n",
    "```\n",
    "all_cols = pl_df_train.columns.tolist()\n",
    "add_cols = [x for x in all_cols if x not in logreg_field_list]\n",
    "add_cols = [x for x in add_cols if x not in ['y_var', 'current_month', 'y2_var']]\n",
    "\n",
    "\n",
    "tb_results = pd.DataFrame(\n",
    "    columns=[\n",
    "        'feature', \n",
    "        'rmse',\n",
    "        'diff_rmse'\n",
    "    ]\n",
    ")\n",
    "\n",
    "tb_results = tb_results.append(\n",
    "    {\n",
    "        'feature': 'base',\n",
    "        'rmse': rmse,\n",
    "        'diff_rmse': 0\n",
    "    },\n",
    "    ignore_index = True\n",
    ")\n",
    "\n",
    "base_rmse = rmse\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop to test each additional variable\n",
    "\n",
    "```\n",
    "for i in add_cols:\n",
    "    print(\"adding the following variable:\", i)\n",
    "    model_field_list = logreg_field_list\n",
    "    model_field_list = model_field_list + [i]\n",
    "    \n",
    "    pl_df_train_x = pl_df_train[model_field_list]\n",
    "    pl_df_train_y = pl_df_train[y_var]\n",
    "\n",
    "    pl_df_test_x = pl_df_test[model_field_list]\n",
    "    pl_df_test_y = pl_df_test[y_var]\n",
    "    \n",
    "    \n",
    "    coefs, rmse, pred_df = logreg_coefs_and_test_rmse(pl_df_train_x, pl_df_train_y, pl_df_test_x, pl_df_test_y)\n",
    "    \n",
    "    tb_results = tb_results.append(\n",
    "        {\n",
    "            'feature': i,\n",
    "            'rmse': rmse,\n",
    "            'diff_rmse': base_rmse - rmse\n",
    "        },\n",
    "        ignore_index = True\n",
    "    )\n",
    "    \n",
    "    print(\"appended result for the following variable:\", i)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See results:\n",
    "\n",
    "```\n",
    "tb_results = tb_results.sort_values(by = 'diff_rmse', ascending = False)\n",
    "\n",
    "tb_results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Add in additional Variable list\n",
    "___\n",
    "Goal: Add in a list of variables that improve the base model RMSE\n",
    "\n",
    "* This list of variables we can pull from the initial list\n",
    "* any feature which has a positive diff RMSE should be added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model with all the variables to see if that helps:\n",
    "\n",
    "```\n",
    "tb_results['feature'][(tb_results['diff_rmse'] > 0) & (tb_results['feature'] != 'y2_var')].to_list()\n",
    "\n",
    "model_field_list = logreg_field_list\n",
    "new_var_list = tb_results['feature'][(tb_results['diff_rmse'] > 0) & (tb_results['feature'] != 'y2_var')].to_list()\n",
    "model_field_list = model_field_list + new_var_list\n",
    "\n",
    "pl_df_train_x = pl_df_train[model_field_list]\n",
    "pl_df_train_y = pl_df_train[y_var]\n",
    "\n",
    "pl_df_test_x = pl_df_test[model_field_list]\n",
    "pl_df_test_y = pl_df_test[y_var]\n",
    "\n",
    "\n",
    "coefs, rmse, pred_df = logreg_coefs_and_test_rmse(pl_df_train_x, pl_df_train_y, pl_df_test_x, pl_df_test_y)\n",
    "\n",
    "tb_results = tb_results.append(\n",
    "    {\n",
    "        'feature': \"all_vars\",\n",
    "        'rmse': rmse,\n",
    "        'diff_rmse': base_rmse - rmse\n",
    "    },\n",
    "    ignore_index = True\n",
    ")\n",
    "\n",
    "tb_results = tb_results.sort_values(by = 'diff_rmse', ascending = False)\n",
    "tb_results[feature = 'all_vars']\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the quantile plot for all variables:\n",
    "\n",
    "```\n",
    "\n",
    "pred_df = pred_df.sort_values(\"pred_y\", ascending = 0).reset_index(drop = True)\n",
    "pred_df['index'] = pred_df.index\n",
    "pred_df['Quantile'] = pd.qcut(pred_df['index'], q = 10)\n",
    "\n",
    "pred_df.groupby('Quantile').agg(\n",
    "    {\n",
    "        'act_y':'sum',\n",
    "        'pred_y':'sum'\n",
    "    }\n",
    ").reset_index(drop = True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
